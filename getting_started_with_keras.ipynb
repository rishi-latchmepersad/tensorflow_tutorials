{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/1Q8P6f/3i2swvp6DkB3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishi-latchmepersad/tensorflow_tutorials/blob/main/getting_started_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Deep Learning Model Families\n",
        "\n",
        "This is a quick overview of the different types of NN layers available today.\n",
        "---\n",
        "\n",
        "## 1) MLP, Feed-Forward networks\n",
        "What: Stacked fully connected layers on vector inputs.\n",
        "\n",
        "Intuition: Each neuron looks at all features at once, learns a weighted sum, then applies a nonlinearity. Deeper stacks learn more abstract combinations.\n",
        "\n",
        "How it works: Every layer takes the previous layer’s numbers, multiplies by a learned weight matrix, adds a bias, then applies something like ReLU. Stacking layers means you can learn complex input to output mappings.\n",
        "\n",
        "When to use: Tabular data, small classifiers, quick baselines.\n",
        "\n",
        "Limits: No built-in notion of space or time. Flattening images or long sequences throws away structure and inflates parameter count.\n",
        "\n",
        "TinyML notes: Good as a small classifier head after a compact encoder. Avoid large Flattened inputs.\n",
        "\n",
        "Typical Keras: Dense → ReLU → Dropout → Dense.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Convolutional Neural Networks (CNNs)\n",
        "\n",
        "### 2D CNNs for images\n",
        "What: Sliding filters that scan across height and width, detecting local patterns like edges, corners, textures, then shapes in deeper layers.\n",
        "\n",
        "Intuition: A small kernel, for example 3 by 3, moves across the image. The same weights are reused everywhere. That reuse makes the model data efficient and sensitive to local structure. Pooling or stride adds some shift invariance.\n",
        "\n",
        "How it works: For each filter, take a small patch under the kernel, multiply by kernel weights, sum, and produce one number. Do this at every location to build a feature map. Multiple filters produce multiple channels. Deeper layers stack more filters, which increases the effective field of view.\n",
        "\n",
        "Key vocabulary:\n",
        "- Stride: how far the window moves each step.\n",
        "- Padding: “same” keeps size, “valid” shrinks it.\n",
        "- Pooling: downsamples by taking max or average inside a window.\n",
        "- Global Average Pooling (GAP): average each channel over the whole spatial grid to get a short vector.\n",
        "\n",
        "When to use: Any image task, including tiny images like MNIST or small gauge crops.\n",
        "\n",
        "TinyML notes: Prefer DepthwiseConv2D and GAP for parameter and compute savings. Keep activations simple, for example ReLU or ReLU6.\n",
        "\n",
        "Typical Keras: [Conv2D → ReLU] one to three times, then Pool, repeat once or twice, then GAP, then Dense softmax.\n",
        "\n",
        "### 1D CNNs for sequences\n",
        "What: Sliding filters along time that capture short patterns and trends in sensor streams or audio features.\n",
        "\n",
        "Intuition: Same idea as 2D but along one axis. Causal padding ensures the model does not peek into the future. Dilations let filters skip steps to see further back without adding many parameters.\n",
        "\n",
        "How it works: A kernel of width k looks at the last k steps and produces one output step. Stacking layers increases temporal coverage. Adding dilation means the kernel samples every second, fourth, eighth step, and so on, which expands reach quickly.\n",
        "\n",
        "When to use: Forecasting, sequence classification for sensors, simple audio tasks.\n",
        "\n",
        "TinyML notes: Often faster and lighter than RNNs for similar accuracy. Uses standard conv ops with strong TFLite Micro support.\n",
        "\n",
        "Typical Keras: Conv1D with padding set to causal → ReLU → optional dilated Conv1D → GAP → Dense.\n",
        "\n",
        "### Depthwise-separable CNNs\n",
        "What: A factorized convolution for efficiency.\n",
        "\n",
        "Intuition: First, a depthwise step applies one small spatial filter per channel. Second, a pointwise 1 by 1 conv mixes channels. This separates spatial and channel mixing.\n",
        "\n",
        "Why it helps: Regular conv does spatial work and channel mixing in one heavy step. Splitting the steps reduces parameters and multiply-adds significantly.\n",
        "\n",
        "TinyML notes: Often the best accuracy per kilobyte for small vision models.\n",
        "\n",
        "Typical Keras block: DepthwiseConv2D → Conv2D with kernel size 1 → ReLU → Pool.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Residual CNNs and inverted residuals\n",
        "What: Convolutional blocks with skip connections or MobileNet-style bottlenecks.\n",
        "\n",
        "Intuition: Learn a residual function and add it back to the input. The identity path keeps gradients healthy, so you can stack more layers without training issues.\n",
        "\n",
        "How it works: A small stack of convs processes the input to produce a residual. Add the residual to the original input, then apply an activation. Inverted residuals expand channels with a 1 by 1 conv, apply a depthwise conv, then project back down with another 1 by 1 conv, often with a linear activation at the end.\n",
        "\n",
        "When to use: When you want to go a bit deeper without losing stability. Very common even in compact networks.\n",
        "\n",
        "TinyML notes: Adds a cheap Add op and keeps two tensors alive until the merge. Usually worth it.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Temporal Convolutional Networks (TCN)\n",
        "What: Sequence models built from causal, dilated Conv1D blocks with residual connections.\n",
        "\n",
        "Intuition: By doubling the dilation each layer, a few layers can see a long history. You keep full parallelism over time and avoid recurrent loops.\n",
        "\n",
        "How it works: Each block uses Conv1D with a chosen dilation rate, often 1, 2, 4, 8, and so on, plus a skip connection. The effective temporal coverage grows quickly with depth.\n",
        "\n",
        "When to use: Forecasting and sequence modeling where you need long context but want fast, stable training.\n",
        "\n",
        "TinyML notes: Uses only conv and add. Very deployment friendly.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) RNNs, LSTM, GRU\n",
        "What: Recurrent models with a learned hidden state that carries information through time.\n",
        "\n",
        "Intuition: At each time step, the model updates a hidden state using the previous state and the current input. LSTM and GRU add gates that decide what to keep, what to forget, and what to write, which helps with long-term dependencies.\n",
        "\n",
        "How it works: A simple RNN updates state with a nonlinearity. LSTM and GRU add learned gates that control the flow of information. This reduces vanishing or exploding gradients compared with a plain RNN.\n",
        "\n",
        "When to use: When true long memory is essential and Conv1D or TCN underperform.\n",
        "\n",
        "Limits: Sequential time steps reduce parallelism. Often heavier than Conv1D for similar context.\n",
        "\n",
        "TinyML notes: TFLite supports LSTM and GRU. TFLite Micro support is improving, but Conv1D or TCN is usually easier.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Transformers\n",
        "What: Models that use self-attention to mix information across all positions in a sequence or across image patches.\n",
        "\n",
        "Intuition: Every position can look at every other position and decide how much to pay attention to it. This gives global context quickly.\n",
        "\n",
        "How it works: Inputs are projected to queries, keys, and values. Attention weights are computed by comparing queries to keys, then applied to values. Multi-head attention runs several of these in parallel. Since attention mixes positions freely, the model adds positional information separately.\n",
        "\n",
        "When to use: Language, Vision Transformers, multimodal tasks. Use only if you really need global mixing or plan to distill back to a small CNN.\n",
        "\n",
        "TinyML notes: Memory heavy. Not a first choice for microcontrollers.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Autoencoders and Variational Autoencoders\n",
        "What: Learn a compact latent code that can reconstruct the input.\n",
        "\n",
        "Intuition: The encoder compresses, the decoder rebuilds. The bottleneck forces the model to keep only the most important information. VAEs treat the latent as a distribution, which regularizes and enables sampling.\n",
        "\n",
        "How it works: Autoencoders minimize a reconstruction loss between input and output. VAEs add a regularization term that keeps the latent distribution close to a simple prior.\n",
        "\n",
        "When to use: Denoising, anomaly detection, learned compression.\n",
        "\n",
        "TinyML notes: Small autoencoders can run on MCUs. Keep the decoder shallow and the latent small.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) GANs\n",
        "What: A generator produces samples and a discriminator tries to tell real from fake. They train together.\n",
        "\n",
        "Intuition: The generator learns to fool the discriminator. The discriminator learns to detect fakes. Each improves the other.\n",
        "\n",
        "When to use: Data synthesis, style transfer, augmentation.\n",
        "\n",
        "Limits: Training is delicate and sensitive to hyperparameters.\n",
        "\n",
        "TinyML notes: Train off device. Generator inference can still be heavy.\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Diffusion models\n",
        "What: Generative models that learn to denoise step by step.\n",
        "\n",
        "Intuition: Start from noise and repeatedly denoise to produce a sample. The model learns the denoising steps during training.\n",
        "\n",
        "When to use: High fidelity image or audio generation. Not for MCUs.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Graph Neural Networks\n",
        "What: Neural nets for graph-structured data such as molecules and road networks.\n",
        "\n",
        "Intuition: Each node gathers messages from neighbors, mixes them with its own state, and updates. After several rounds, each node encodes information from a larger neighborhood.\n",
        "\n",
        "How it works: A message function produces neighbor messages, an aggregation function combines them, then a small MLP updates the node state. Repeat across layers.\n",
        "\n",
        "When to use: When relationships are best represented as a graph.\n",
        "\n",
        "TinyML notes: Irregular structures and custom ops make deployment hard today.\n"
      ],
      "metadata": {
        "id": "egdh2b1sR0PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install/upgrade the packages we need\n",
        "!pip install --upgrade keras\n",
        "!pip install --upgrade keras-cv\n",
        "!pip install --upgrade keras-hub\n",
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czOHhaSf4DsR",
        "outputId": "09dc5bb1-3ac8-49ac-84f9-e381c66468ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "Successfully installed keras-3.11.3\n",
            "Collecting keras-cv\n",
            "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-cv) (25.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras-cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from keras-cv) (2024.11.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.12/dist-packages (from keras-cv) (4.9.9)\n",
            "Collecting keras-core (from keras-cv)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from keras-cv) (0.3.13)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-cv) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-cv) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-cv) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras-core->keras-cv) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras-core->keras-cv) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras-core->keras-cv) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras-core->keras-cv) (3.14.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from keras-core->keras-cv) (0.1.9)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (0.8.1)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (5.29.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (3.1.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets->keras-cv) (1.17.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (4.15.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-cv) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-cv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-cv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-cv) (2025.8.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree->keras-core->keras-cv) (25.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from promise->tensorflow-datasets->keras-cv) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-core->keras-cv) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-core->keras-cv) (2.19.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from simple_parsing->tensorflow-datasets->keras-cv) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.70.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
            "Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-core, keras-cv\n",
            "Successfully installed keras-core-0.1.7 keras-cv-0.9.0\n",
            "Requirement already satisfied: keras-hub in /usr/local/lib/python3.12/dist-packages (0.21.1)\n",
            "Collecting keras-hub\n",
            "  Downloading keras_hub-0.22.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: keras>=3.8 in /usr/local/lib/python3.12/dist-packages (from keras-hub) (3.11.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras-hub) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras-hub) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-hub) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from keras-hub) (2024.11.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras-hub) (13.9.4)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from keras-hub) (0.3.13)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.12/dist-packages (from keras-hub) (2.19.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.8->keras-hub) (0.5.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub->keras-hub) (4.67.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-hub) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras-hub) (2.19.2)\n",
            "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-text->keras-hub) (2.19.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras-hub) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (1.75.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (2.19.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->keras-hub) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (0.45.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub) (3.0.2)\n",
            "Downloading keras_hub-0.22.2-py3-none-any.whl (947 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m947.9/947.9 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-hub\n",
            "  Attempting uninstall: keras-hub\n",
            "    Found existing installation: keras-hub 0.21.1\n",
            "    Uninstalling keras-hub-0.21.1:\n",
            "      Successfully uninstalled keras-hub-0.21.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.21.1 requires keras-hub==0.21.1, but you have keras-hub 0.22.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-hub-0.22.2\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.11.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7_-5gbTgC5pB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras import layers as L\n",
        "keras.utils.set_random_seed(42) # set seed to ensure reproducibility\n",
        "from keras import ops\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Dict, Iterable, Union\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the mnist dataset is a large dataset (60000 images) of handwritten digits from 0-9, commonly used to evaluate computer vision tasks\n",
        "# we first load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# let's plot 10 random images from our test dataset, to see what we're trying to predict\n",
        "\n",
        "# first we sample 10 random indices from the test set\n",
        "indices = np.random.choice(len(x_test), size=10, replace=False)\n",
        "\n",
        "# then we grab the images and labels for those indices.\n",
        "imgs   = x_test[indices]      # shape (10, H, W, C) or (10, H, W)\n",
        "labels = y_test[indices]      # shape (10,)\n",
        "\n",
        "# ensure images are 4D for prediction (N, H, W, C). If they’re (N, H, W) add the channel axis.\n",
        "if imgs.ndim == 3:\n",
        "    imgs = imgs[..., None]\n",
        "\n",
        "# then plot a 2×5 grid\n",
        "rows, cols = 2, 5\n",
        "plt.figure(figsize=(cols * 2.2, rows * 2.2))\n",
        "for i, (img, y_true) in enumerate(zip(imgs, labels)):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "    # Display: grayscale if single channel, RGB otherwise\n",
        "    if img.shape[-1] == 1:\n",
        "        # Pick display range based on dtype for nicer contrast\n",
        "        vmin, vmax = (0, 255) if img.dtype == np.uint8 else (0, 1)\n",
        "        plt.imshow(img.squeeze(), cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
        "    else:\n",
        "        plt.imshow(img)\n",
        "\n",
        "    title = f\"label {int(y_true)}\"\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Random test samples\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "Wrse_ozrbODg",
        "outputId": "bff585f4-8a1b-40b7-e4b9-12abba619815"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x440 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's process our dataset to prepare for training our model\n",
        "# normalize the images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# the mnist dataset provides a set of 28x28 greyscale images, but it doesn't explicitly set the number of channels to be 1 (greyscale)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "# so we add the extra dimension to the end for use in the later convolution layers\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", x_test.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHc_8b6YIvvy",
        "outputId": "92fdf2a0-a731-43ab-b899-7f11b3f5da31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (60000, 28, 28)\n",
            "x_test shape: (10000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "y_train shape: (10000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we build our model to predict the digit using the keras functional API\n",
        "def build_functional_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Build a small VGG-style CNN using the keras functional API.\n",
        "    input_shape: tuple (H, W, C), for MNIST (usually (28, 28, 1))\n",
        "    num_classes: number of output classes for classification (usually 10 for MNIST)\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=input_shape)         # Define the symbolic input tensor for the computation graph\n",
        "\n",
        "    # First convolutional block: two 3×3 convs and a max pooling layer\n",
        "    # Note: padding='valid' is the default and shrinks H and W by 2 each conv.\n",
        "    # If we want to keep spatial size, set padding='same'.\n",
        "    x = L.Conv2D(64, (3, 3), activation=\"relu\")(inputs)   # Extract local features with 64 filters, add nonlinearity\n",
        "    x = L.Conv2D(64, (3, 3), activation=\"relu\")(x)        # Stack another 3×3 to expand receptive field with modest params\n",
        "    x = L.MaxPooling2D((2, 2))(x)                         # Downsample by 2, reduce compute, gain some translation invariance\n",
        "\n",
        "    # Second convolutional block: two more 3x3 convs\n",
        "    # increase channel depth as resolution drops\n",
        "    x = L.Conv2D(128, (3, 3), activation=\"relu\")(x)       # Learn richer features at lower spatial resolution\n",
        "    x = L.Conv2D(128, (3, 3), activation=\"relu\")(x)       # Another 3×3 for more expressive power without huge kernels\n",
        "\n",
        "    # Finally, our classifier head: make features compact, regularize, then classify\n",
        "    x = L.GlobalAveragePooling2D()(x)                     # Average each feature map over H and W, get a 128-D vector; avoids large Dense layers\n",
        "    x = L.Dropout(0.5)(x)                                 # Randomly drop activations during training, reduce overfitting; inactive at inference\n",
        "\n",
        "    outputs = L.Dense(num_classes, activation=\"softmax\")(x)  # Map to class probabilities for single-label multiclass tasks\n",
        "\n",
        "    return keras.Model(inputs=inputs,                      # Assemble inputs and outputs into a Model object\n",
        "                       outputs=outputs,\n",
        "                       name=\"mnist_predict_digits\")\n"
      ],
      "metadata": {
        "id": "KMu09wf2R4dU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the function to build the model. we use 10 classes since we have digits 0-9, and use the first training element to set the input shape\n",
        "model:keras.Model = build_functional_model(input_shape=x_train.shape[1:], num_classes=10)\n",
        "# compile the model to set the rules for fitting and evaluating the model.\n",
        "# adam is usually a good default optimizer (it uses SGD)\n",
        "# we use sparse categorical crossentropy for the loss since we have multiple output classes which are integers (0-9)\n",
        "# and we want to report on accuracy\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "## plot the model graph if we need it\n",
        "# keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "olvWx3ACPG3K",
        "outputId": "fb151598-970f-4898-ecac-805318282c83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"mnist_predict_digits\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mnist_predict_digits\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m260,298\u001b[0m (1016.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">260,298</span> (1016.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m260,298\u001b[0m (1016.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">260,298</span> (1016.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we actually train our model using our test data\n",
        "# first we define callbacks that make training safer and faster.\n",
        "callbacks = [\n",
        "    # EarlyStopping watches validation loss; if it stops improving for 'patience' epochs,\n",
        "    # training halts early to prevent overfitting and to save time.\n",
        "    # restore_best_weights=True puts the model back to the epoch with the lowest val_loss.\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",     # default; explicit for clarity\n",
        "        patience=5,             # wait 5 epochs without improvement\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "\n",
        "    # ReduceLROnPlateau lowers the learning rate when validation loss stalls.\n",
        "    # This often lets the optimizer settle into a better minimum after the fast phase.\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\",\n",
        "        factor=0.5,             # cut LR in half each time the plateau is detected\n",
        "        patience=2,             # wait 2 stagnant epochs before reducing LR\n",
        "        min_lr=1e-5             # do not shrink below this floor\n",
        "    ),\n",
        "]\n",
        "\n",
        "# then we train/fit the model.\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_split=0.1,       # hold out 10% of training data for validation;\n",
        "                                # callbacks look at this, not the test set, to avoid leakage\n",
        "    epochs=20,                  # maximum number of passes; EarlyStopping may stop earlier\n",
        "    batch_size=128,             # tradeoff between speed and generalization; 128 is a solid default\n",
        "    callbacks=callbacks,               # attach the callbacks defined above\n",
        "    verbose=2                   # concise logs per epoch; change to 1 or 0 if you prefer\n",
        ")\n",
        "\n",
        "# then evaluate once on the unseen test set.\n",
        "# This gives an unbiased estimate of generalization, since we did not peek at it during training.\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# finally report results for our model in a compact way.\n",
        "print(f\"Test loss: {test_loss:.4f}  |  Test accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU1cbpp8V3uT",
        "outputId": "775cf833-3c78-4622-e226-5596a0797646"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "422/422 - 20s - 47ms/step - accuracy: 0.7507 - loss: 0.7329 - val_accuracy: 0.9593 - val_loss: 0.1547 - learning_rate: 1.0000e-03\n",
            "Epoch 2/20\n",
            "422/422 - 8s - 19ms/step - accuracy: 0.9396 - loss: 0.2016 - val_accuracy: 0.9802 - val_loss: 0.0766 - learning_rate: 1.0000e-03\n",
            "Epoch 3/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9591 - loss: 0.1373 - val_accuracy: 0.9847 - val_loss: 0.0568 - learning_rate: 1.0000e-03\n",
            "Epoch 4/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9663 - loss: 0.1149 - val_accuracy: 0.9857 - val_loss: 0.0505 - learning_rate: 1.0000e-03\n",
            "Epoch 5/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9725 - loss: 0.0965 - val_accuracy: 0.9888 - val_loss: 0.0396 - learning_rate: 1.0000e-03\n",
            "Epoch 6/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9762 - loss: 0.0807 - val_accuracy: 0.9897 - val_loss: 0.0371 - learning_rate: 1.0000e-03\n",
            "Epoch 7/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9776 - loss: 0.0741 - val_accuracy: 0.9907 - val_loss: 0.0341 - learning_rate: 1.0000e-03\n",
            "Epoch 8/20\n",
            "422/422 - 6s - 14ms/step - accuracy: 0.9795 - loss: 0.0690 - val_accuracy: 0.9912 - val_loss: 0.0311 - learning_rate: 1.0000e-03\n",
            "Epoch 9/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9828 - loss: 0.0591 - val_accuracy: 0.9917 - val_loss: 0.0324 - learning_rate: 1.0000e-03\n",
            "Epoch 10/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9825 - loss: 0.0590 - val_accuracy: 0.9923 - val_loss: 0.0270 - learning_rate: 1.0000e-03\n",
            "Epoch 11/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9844 - loss: 0.0537 - val_accuracy: 0.9927 - val_loss: 0.0270 - learning_rate: 1.0000e-03\n",
            "Epoch 12/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9850 - loss: 0.0486 - val_accuracy: 0.9938 - val_loss: 0.0246 - learning_rate: 1.0000e-03\n",
            "Epoch 13/20\n",
            "422/422 - 5s - 12ms/step - accuracy: 0.9864 - loss: 0.0451 - val_accuracy: 0.9932 - val_loss: 0.0273 - learning_rate: 1.0000e-03\n",
            "Epoch 14/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9871 - loss: 0.0440 - val_accuracy: 0.9918 - val_loss: 0.0303 - learning_rate: 1.0000e-03\n",
            "Epoch 15/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9896 - loss: 0.0334 - val_accuracy: 0.9932 - val_loss: 0.0275 - learning_rate: 5.0000e-04\n",
            "Epoch 16/20\n",
            "422/422 - 5s - 12ms/step - accuracy: 0.9901 - loss: 0.0326 - val_accuracy: 0.9943 - val_loss: 0.0233 - learning_rate: 5.0000e-04\n",
            "Epoch 17/20\n",
            "422/422 - 5s - 11ms/step - accuracy: 0.9904 - loss: 0.0314 - val_accuracy: 0.9938 - val_loss: 0.0236 - learning_rate: 5.0000e-04\n",
            "Epoch 18/20\n",
            "422/422 - 5s - 12ms/step - accuracy: 0.9911 - loss: 0.0292 - val_accuracy: 0.9937 - val_loss: 0.0250 - learning_rate: 5.0000e-04\n",
            "Epoch 19/20\n",
            "422/422 - 5s - 12ms/step - accuracy: 0.9924 - loss: 0.0255 - val_accuracy: 0.9942 - val_loss: 0.0252 - learning_rate: 2.5000e-04\n",
            "Epoch 20/20\n",
            "422/422 - 5s - 12ms/step - accuracy: 0.9923 - loss: 0.0246 - val_accuracy: 0.9942 - val_loss: 0.0249 - learning_rate: 2.5000e-04\n",
            "Test loss: 0.0207  |  Test accuracy: 0.9937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot 10 images from our test dataset to help us visualize how our model is performing\n",
        "\n",
        "# first we sample 10 random indices from the test set\n",
        "indices = np.random.choice(len(x_test), size=10, replace=False)\n",
        "\n",
        "# then we grab the images and labels for those indices.\n",
        "imgs   = x_test[indices]      # shape (10, H, W, C) or (10, H, W)\n",
        "labels = y_test[indices]      # shape (10,)\n",
        "\n",
        "# ensure images are 4D for prediction (N, H, W, C). If they’re (N, H, W) add the channel axis.\n",
        "if imgs.ndim == 3:\n",
        "    imgs = imgs[..., None]\n",
        "\n",
        "# then run the model to get class probabilities, and take argmax → predicted class ID per image.\n",
        "probs = model.predict(imgs, verbose=0)      # shape (10, num_classes)\n",
        "preds = probs.argmax(axis=1)\n",
        "\n",
        "# then plot a 2×5 grid, labeling each tile with the predicted class (and the true one if wrong).\n",
        "rows, cols = 2, 5\n",
        "plt.figure(figsize=(cols * 2.2, rows * 2.2))\n",
        "for i, (img, y_true, y_pred) in enumerate(zip(imgs, labels, preds)):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "    # Display: grayscale if single channel, RGB otherwise\n",
        "    if img.shape[-1] == 1:\n",
        "        # Pick display range based on dtype for nicer contrast\n",
        "        vmin, vmax = (0, 255) if img.dtype == np.uint8 else (0, 1)\n",
        "        plt.imshow(img.squeeze(), cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
        "    else:\n",
        "        plt.imshow(img)\n",
        "\n",
        "    ok = int(y_true) == int(y_pred)\n",
        "    title = f\"pred {int(y_pred)}\" if ok else f\"pred {int(y_pred)} (true {int(y_true)})\"\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Random test samples with model predictions\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "Tmec17ekaoOo",
        "outputId": "07d035be-c374-462e-a3e2-d34e9fa6d65a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x440 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's shrink this model using tflite\n",
        "\n",
        "# first set up some useful helper functions\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helpers: readable sizes, accuracy functions, and TFLite memory estimation\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def bytes_to_readable(num_bytes: int) -> str:\n",
        "    \"\"\"\n",
        "    Convert a byte count into a human-friendly string.\n",
        "    Why: quick comparison of model footprints without mental math.\n",
        "    \"\"\"\n",
        "    if num_bytes < 1024:\n",
        "        return f\"{num_bytes} B\"\n",
        "    kb = num_bytes / 1024.0\n",
        "    if kb < 1024:\n",
        "        return f\"{kb:.1f} KB\"\n",
        "    mb = kb / 1024.0\n",
        "    return f\"{mb:.2f} MB\"\n",
        "\n",
        "def compute_keras_accuracy(\n",
        "    keras_model: keras.Model,\n",
        "    inputs: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    batch_size: int = 512\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Accuracy from the full Keras model via predict + argmax.\n",
        "    Why: matches the TFLite evaluation style for a fair comparison.\n",
        "    \"\"\"\n",
        "    predicted_class_ids: np.ndarray = keras_model.predict(\n",
        "        inputs, batch_size=batch_size, verbose=0\n",
        "    ).argmax(axis=1)\n",
        "    return float((predicted_class_ids == labels).mean())\n",
        "\n",
        "def compute_tflite_accuracy(\n",
        "    tflite_model: Union[str, bytes, bytearray],\n",
        "    inputs: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    number_of_examples: Optional[int] = None\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Accuracy from a TFLite model (float or quantized).\n",
        "    Auto-quantizes the input batch to uint8/int8 if needed based on model metadata.\n",
        "    Why: compare deployment model accuracy to the original Keras model.\n",
        "    \"\"\"\n",
        "    interpreter: tf.lite.Interpreter = (\n",
        "        tf.lite.Interpreter(model_content=tflite_model)\n",
        "        if isinstance(tflite_model, (bytes, bytearray))\n",
        "        else tf.lite.Interpreter(model_path=tflite_model)\n",
        "    )\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details: Dict = interpreter.get_input_details()[0]\n",
        "    output_details: Dict = interpreter.get_output_details()[0]\n",
        "\n",
        "    input_dtype = input_details[\"dtype\"]\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "\n",
        "    total: int = len(inputs) if number_of_examples is None else min(number_of_examples, len(inputs))\n",
        "    correct: int = 0\n",
        "\n",
        "    for index in range(total):\n",
        "        single_input: np.ndarray = inputs[index:index+1]  # keep batch dimension\n",
        "\n",
        "        # Match expected input dtype of the TFLite model.\n",
        "        if input_dtype == np.float32:\n",
        "            single_input_feed: np.ndarray = single_input.astype(np.float32)\n",
        "        elif input_dtype == np.uint8:\n",
        "            # Map float [0,1] to quantized [0,255] using scale and zero-point from the model.\n",
        "            single_input_feed = np.clip(\n",
        "                np.round(single_input / input_scale + input_zero_point), 0, 255\n",
        "            ).astype(np.uint8)\n",
        "        elif input_dtype == np.int8:\n",
        "            # Map float [0,1] to quantized [-128,127].\n",
        "            single_input_feed = np.clip(\n",
        "                np.round(single_input / input_scale + input_zero_point), -128, 127\n",
        "            ).astype(np.int8)\n",
        "        else:\n",
        "            raise TypeError(f\"Unsupported TFLite input dtype: {input_dtype}\")\n",
        "\n",
        "        interpreter.set_tensor(input_details[\"index\"], single_input_feed)\n",
        "        interpreter.invoke()\n",
        "        logits_or_scores: np.ndarray = interpreter.get_tensor(output_details[\"index\"])  # shape (1, num_classes)\n",
        "        predicted_class_id: int = int(np.argmax(logits_or_scores, axis=1)[0])\n",
        "        correct += int(predicted_class_id == int(labels[index]))\n",
        "\n",
        "    return correct / float(total)\n",
        "\n",
        "def _dtype_nbytes(np_dtype: np.dtype) -> int:\n",
        "    \"\"\"Map common NumPy dtypes to byte sizes.\"\"\"\n",
        "    if np_dtype == np.float32: return 4\n",
        "    if np_dtype == np.float16: return 2\n",
        "    if np_dtype == np.int8:    return 1\n",
        "    if np_dtype == np.uint8:   return 1\n",
        "    if np_dtype == np.int16:   return 2\n",
        "    if np_dtype == np.int32:   return 4\n",
        "    # default conservative\n",
        "    return np.dtype(np_dtype).itemsize\n",
        "\n",
        "def estimate_tflite_tensor_memory_bytes(interpreter: tf.lite.Interpreter) -> int:\n",
        "    \"\"\"\n",
        "    Estimate total bytes of all allocated tensors in the TFLite interpreter.\n",
        "    Why: gives a rough sense of RAM usage during inference (weights + activations).\n",
        "    Note: This is an approximation for desktop TFLite; TFLite Micro uses a manual arena.\n",
        "    \"\"\"\n",
        "    total_bytes: int = 0\n",
        "    for detail in interpreter.get_tensor_details():\n",
        "        shape = detail.get(\"shape_signature\", detail.get(\"shape\", None))\n",
        "        if shape is None or -1 in shape:\n",
        "            # Skip dynamic shapes that are not allocated yet.\n",
        "            continue\n",
        "        num_elements = int(np.prod(shape)) if len(shape) > 0 else 1\n",
        "        dtype_bytes = _dtype_nbytes(detail[\"dtype\"])\n",
        "        total_bytes += num_elements * dtype_bytes\n",
        "    return total_bytes\n",
        "\n",
        "def representative_dataset() -> Iterable[List[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Yield small batches of real training samples (as float32) to calibrate int8 ranges.\n",
        "    Why: representative data is critical for good int8 accuracy.\n",
        "    Format requirement: yield [sample_array]  (a LIST), not (sample_array,) (a TUPLE).\n",
        "    \"\"\"\n",
        "    calibration_count: int = min(200, len(x_train))  # ~200 is plenty for MNIST\n",
        "    for i in range(calibration_count):\n",
        "        # Each sample must match the model input dtype/shape, typically float32 in [0, 1]\n",
        "        sample: np.ndarray = x_train[i:i+1].astype(np.float32)  # shape (1, 28, 28, 1)\n",
        "        yield [sample]\n"
      ],
      "metadata": {
        "id": "bbPYpVh-8rNa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now actually create the tflite models\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Create TFLite models (float and full-int8) and compute accuracy + sizes\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# save the full Keras model so we can compare its file size.\n",
        "keras_model_path: str = \"model_full.keras\"\n",
        "if not os.path.exists(keras_model_path):\n",
        "    # the modern .keras format is portable across Keras backends and saves space.\n",
        "    model.save(keras_model_path)\n",
        "keras_model_size_bytes: int = os.path.getsize(keras_model_path)\n",
        "\n",
        "# export a float TFLite model for a clean apples-to-apples accuracy check.\n",
        "tflite_float_path: str = \"model_float.tflite\"\n",
        "if not os.path.exists(tflite_float_path):\n",
        "    float_converter: tf.lite.TFLiteConverter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    tflite_float_bytes: bytes = float_converter.convert()\n",
        "    with open(tflite_float_path, \"wb\") as file_handle:\n",
        "        file_handle.write(tflite_float_bytes)\n",
        "tflite_float_size_bytes: int = os.path.getsize(tflite_float_path)\n",
        "\n",
        "# export a fully quantized INT8 TFLite model (weights and activations).\n",
        "# this is what we’ll deploy on MCUs or small CPUs; it shrinks size and speeds up inference.\n",
        "def representative_dataset() -> Iterable[Tuple[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Yield small batches of real training samples to calibrate int8 ranges.\n",
        "    representative data is critical for good int8 accuracy.\n",
        "    \"\"\"\n",
        "    # Use ~200 samples for MNIST; increase for tougher datasets.\n",
        "    calibration_count: int = min(200, len(x_train))\n",
        "    for i in range(calibration_count):\n",
        "        # Each yield must be a tuple whose first element matches the model input shape.\n",
        "        yield (x_train[i:i+1].astype(np.float32),)\n",
        "\n",
        "tflite_int8_path: str = \"model_int8.tflite\"\n",
        "if not os.path.exists(tflite_int8_path):\n",
        "    int8_converter: tf.lite.TFLiteConverter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "    # Enable post-training quantization.\n",
        "    int8_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "    # Provide calibration samples (the fix above ensures correct format).\n",
        "    int8_converter.representative_dataset = representative_dataset\n",
        "\n",
        "    # Force full-integer kernels so it runs on MCUs and edge CPUs without float ops.\n",
        "    int8_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "    # Choose deployment-friendly I/O types. Keep inputs/outputs uint8 for widest compatibility.\n",
        "    int8_converter.inference_input_type = tf.uint8\n",
        "    int8_converter.inference_output_type = tf.uint8\n",
        "\n",
        "    # Convert and save.\n",
        "    tflite_int8_bytes: bytes = int8_converter.convert()\n",
        "    with open(tflite_int8_path, \"wb\") as file_handle:\n",
        "        file_handle.write(tflite_int8_bytes)\n",
        "tflite_int8_size_bytes: int = os.path.getsize(tflite_int8_path)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Evaluate accuracy and estimate memory\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "keras_accuracy_value: float = compute_keras_accuracy(model, x_test, y_test)\n",
        "\n",
        "tflite_float_interpreter: tf.lite.Interpreter = tf.lite.Interpreter(model_path=tflite_float_path)\n",
        "tflite_float_interpreter.allocate_tensors()\n",
        "tflite_float_accuracy_value: float = compute_tflite_accuracy(tflite_float_path, x_test, y_test)\n",
        "tflite_float_tensor_bytes: int = estimate_tflite_tensor_memory_bytes(tflite_float_interpreter)\n",
        "\n",
        "tflite_int8_interpreter: tf.lite.Interpreter = tf.lite.Interpreter(model_path=tflite_int8_path)\n",
        "tflite_int8_interpreter.allocate_tensors()\n",
        "tflite_int8_accuracy_value: float = compute_tflite_accuracy(tflite_int8_path, x_test, y_test)\n",
        "tflite_int8_tensor_bytes: int = estimate_tflite_tensor_memory_bytes(tflite_int8_interpreter)\n",
        "\n",
        "# Keras parameter count and an approximate weight memory if stored as float32.\n",
        "total_params: int = int(model.count_params())\n",
        "approx_keras_weight_bytes: int = total_params * 4  # float32 = 4 bytes\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Print a compact comparison table\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n — Accuracy and space comparison between full keras model and tflite model\")\n",
        "print(\"-\" * 72)\n",
        "print(f\"{'Model':<18} | {'Accuracy':>8} | {'File size':>12} | {'RAM est. (tensors)':>18}\")\n",
        "print(\"-\" * 72)\n",
        "print(f\"{'Keras (float32)':<18} | {keras_accuracy_value:8.4f} | \"\n",
        "      f\"{bytes_to_readable(keras_model_size_bytes):>12} | \"\n",
        "      f\"{bytes_to_readable(approx_keras_weight_bytes):>18}  # approx weights only\")\n",
        "print(f\"{'TFLite (float32)':<18} | {tflite_float_accuracy_value:8.4f} | \"\n",
        "      f\"{bytes_to_readable(tflite_float_size_bytes):>12} | \"\n",
        "      f\"{bytes_to_readable(tflite_float_tensor_bytes):>18}\")\n",
        "print(f\"{'TFLite (int8)':<18} | {tflite_int8_accuracy_value:8.4f} | \"\n",
        "      f\"{bytes_to_readable(tflite_int8_size_bytes):>12} | \"\n",
        "      f\"{bytes_to_readable(tflite_int8_tensor_bytes):>18}\")\n",
        "print(\"-\" * 72)\n",
        "\n",
        "# Notes:\n",
        "# • The Keras RAM figure is a rough \"weights in float32\" approximation (activations not included).\n",
        "# • The TFLite RAM figure is an approximation of all allocated tensors in the interpreter\n",
        "#   (includes weights and activations). TFLite Micro uses a static arena, but this still\n",
        "#   gives you a useful ballpark comparison across models.\n",
        "# • A small accuracy gap for INT8 vs float is expected. If the gap is large, use a larger or\n",
        "#   more representative calibration set in representative_dataset()."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "aN79Wv7XA_W-",
        "outputId": "9ec7328f-6fca-488e-d79e-7a6b38f8157e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp7vegvvd1'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132760089128272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132760089129232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132760089129808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132760089128848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132760089130960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132760089130000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132760089130576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132760089130192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132760089132304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132760089132880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-395034328.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Convert and save.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtflite_int8_bytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint8_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_int8_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_int8_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1766\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m     \"\"\"\n\u001b[0;32m-> 1768\u001b[0;31m     \u001b[0msaved_model_convert_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_as_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m       )\n\u001b[1;32m   1748\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         return super(TFLiteKerasModelConverterV2, self).convert(\n\u001b[0m\u001b[1;32m   1750\u001b[0m             \u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1492\u001b[0m     )\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m     return self._optimize_tflite_model(\n\u001b[0m\u001b[1;32m   1495\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quant_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_optimize_tflite_model\u001b[0;34m(self, model, quant_mode, debug_options, quant_io)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0mq_allow_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allow_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0mq_variable_quantization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_mlir_variable_quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m         model = self._quantize(\n\u001b[0m\u001b[1;32m   1147\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0mq_in_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization, debug_options)\u001b[0m\n\u001b[1;32m    758\u001b[0m     )\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_calibrate_only\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_new_quantizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m       calibrated = calibrate_quantize.calibrate(\n\u001b[0m\u001b[1;32m    761\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36mcalibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mdataset_gen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mgenerates\u001b[0m \u001b[0mcalibration\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \"\"\"\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalibrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36m_feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m           raise ValueError(\n\u001b[1;32m    105\u001b[0m               \u001b[0;34m\"You need to provide either a dictionary with input \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    }
  ]
}